import pandas as pd
import numpy as np
import time
import random
import streamlit as st

# plots
import seaborn as sns
import matplotlib.pyplot as plt

# HAC
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score

# HDBSCAN
import hdbscan
from hdbscan.validity import validity_index

# FeatureEncode
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import LabelEncoder
#from category_encoders import CatBoostEncoder

# Feature weights
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression

# Cluster Explanation
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree

from tempfile import NamedTemporaryFile
from supertree import SuperTree
from streamlit.components.v1 import html
import faiss

# –†–∞—Å—á–µ—Ç —Ä–µ–Ω–∂–µ–π –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_numeric_ranges(df, num_cols, method='minmax'):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {col: range} –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      - 'minmax'  : range = max - min
      - 'iqr' : range = (Q3-Q1)
    """
    ranges = {}
    for c in num_cols:
        col = df[c].dropna().to_numpy(dtype=float)
        if col.size == 0:
            ranges[c] = 1.0
            continue

        if method == 'minmax':
            r = float(np.max(col) - np.min(col))
        elif method == 'iqr':
            q1 = np.percentile(col, 25)
            q3 = np.percentile(col, 75)
            r = float((q3 - q1))
        else:
            raise ValueError("unknown method")

        # –ï—Å–ª–∏ range —Å–ª–∏—à–∫–æ–º –º–∞–ª, —Ç–æ —Å—Ç–∞–≤–∏–º –µ–≥–æ 1.0
        if not np.isfinite(r) or r <= 1e-8:
            r = 1.0
        ranges[c] = r
    return ranges

# –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_feature_weights(df, target, num_cols, cat_cols):
    """
    –°—á–∏—Ç–∞–µ—Ç –≤–µ—Å–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (MI) —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
    df : –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    target : pd.Series –∏–ª–∏ np.array —Ç–∞—Ä–≥–µ—Ç (—á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π)

    output:
    weights : dict
        {column_name: weight}, –≤–µ—Å–∞ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã (—Å—É–º–º–∞ = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).
    """
    y = target.values

    # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    df_enc = df.copy()
    discrete_features = []
    for c in cat_cols:
        le = LabelEncoder()
        df_enc[c] = le.fit_transform(df_enc[c].astype(str))
        discrete_features.append(df.columns.get_loc(c))  # –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    X = df_enc.to_numpy()

    # –í—ã—á–∏—Å–ª—è–µ–º MI
    if pd.api.types.is_numeric_dtype(target) == True:
        mi = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=42)
    elif pd.api.types.is_object_dtype(target) == True:
        mi = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=42)
    else:
        raise ValueError("–¢–∏–ø —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º")

    # –ù–æ—Ä–º–∏—Ä—É–µ–º –≤–µ—Å–∞ (—Å—É–º–º–∞ = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    mi = np.maximum(mi, 1e-9)  # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –Ω—É–ª–µ–≤—ã—Ö
    mi_normalized = mi / mi.sum() * len(mi)

    weights = {col: w for col, w in zip(df.columns, mi_normalized)}
    return weights

# –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_gower_matrix(df, num_cols, cat_cols, numeric_ranges=None,
                          weights=None, dtype=np.float32):
    """
    - numeric_ranges: dict col->range. –ï—Å–ª–∏ None, computed by minmax on df.
    - weights: dict col->weight. –ï—Å–ª–∏ None - –≤—Å–µ 1
    """
    X = df.reset_index(drop=True)
    n = len(X)
    if n == 0:
        return np.zeros((0,0), dtype=dtype)

    if numeric_ranges is None:
        numeric_ranges = compute_numeric_ranges(X, num_cols, method='minmax')

    D = np.zeros((n, n), dtype=np.float64)  # accumulate in float64 for numeric stability

    # weights
    if weights is None:
        # equal weight per feature
        w_num = {c: 1.0 for c in num_cols}
        w_cat = {c: 1.0 for c in cat_cols}
    else:
        # user-provided; missing keys default to 1.0
        w_num = {c: float(weights.get(c, 1.0)) for c in num_cols}
        w_cat = {c: float(weights.get(c, 1.0)) for c in cat_cols}

    # numeric part
    for c in num_cols:
        col = X[c].to_numpy(dtype=float)
        rng = numeric_ranges.get(c, 1.0)
        # normalized differences (broadcast)
        mat = np.abs(col[:, None] - col[None, :]) / rng
        D += w_num.get(c, 1.0) * mat

    # categorical part
    for c in cat_cols:
        col = X[c].astype(str).to_numpy()
        neq = (col[:, None] != col[None, :]).astype(np.float64)
        D += w_cat.get(c, 1.0) * neq

    # normalize by total weights sum
    total_weight = sum(w_num.values()) + sum(w_cat.values())
    if total_weight <= 0:
        total_weight = 1.0
    D = (D / float(total_weight)).astype(dtype)
    return D.astype(np.float64)

### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
def clusterize(df, D, method="HDBSCAN", max_k=20):
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ.
    method = "hdbscan" –∏–ª–∏ "hac"
    """

    if method == "HDBSCAN":
      best_score = -1
      best_params = None
      best_labels = None
      d = df.shape[1]
      # specify parameters and distributions to sample from
      param_grid = [
          {"min_cluster_size": mcs, "min_samples": ms}
          for mcs in [10, 20, 30, 50]
          for ms in [10, 20, 30, 50]
      ]

      for params in param_grid:
        cl = hdbscan.HDBSCAN(metric="precomputed", **params)
        labels = cl.fit_predict(D)

        if len(set(labels)) > 1:
            score = validity_index(
                D,
                labels,
                metric="precomputed",
                d=d
            )
            if score > best_score:
                best_score = score
                best_params = params
                best_labels = labels

    elif method == "HAC":
      # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤ condensed form
      condensed = squareform(D, checks=False)

      # linkage (average linkage –ª—É—á—à–µ –¥–ª—è Gower)
      Z = linkage(condensed, method="complete")

      # –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k –ø–æ silhouette
      best_k, best_score, best_labels = None, -1, None
      for k in range(2, min(max_k, len(df)//10)):
          labels = fcluster(Z, k, criterion="maxclust")
          if len(np.unique(labels)) < 2:
              continue
          try:
              score = silhouette_score(D, labels, metric="precomputed")
          except Exception:
              continue
          if score > best_score:
              best_score, best_k, best_labels = score, k, labels

      if best_labels is None:
          # fallback: –≤—Å—ë –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
          best_labels = np.ones(len(df), dtype=int)

      labels = best_labels

    else:
      raise ValueError("method must be 'HDBSCAN' or 'HAC'")

    return labels

### –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAISS
def vectorize(df, num_cols, cat_cols, ohe_thresh=20, hash_dim=32):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –≤ —á–∏—Å–ª–æ–≤–æ–π –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ / –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–µ–π.

    Parameters:
        df : pd.DataFrame
        numeric_cols : list[str] - —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        cat_cols : list[str] - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        ohe_thresh : int - –º–∞–∫—Å —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è OHE
        hash_dim : int - —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è FeatureHasher

    output:
        X : np.ndarray, float32
    """
    # --- —á–∏—Å–ª–æ–≤—ã–µ ---
    scaler = RobustScaler()
    X_num = scaler.fit_transform(df[num_cols]) if num_cols else None

    # --- –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ---
    X_cat_list = []
    encoders = {}

    for c in cat_cols:
        n_unique = df[c].nunique()
        col_data = df[[c]].astype(str)

        if n_unique <= ohe_thresh:
            # OHE
            enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            X_c = enc.fit_transform(col_data)
            encoders[c] = enc
        else:
            # FeatureHasher
            enc = FeatureHasher(n_features=hash_dim, input_type='string')
            # FeatureHasher –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: [{cat_val: 1}, ...]
            X_c = enc.transform([{val: 1} for val in col_data[c]]).toarray()
            encoders[c] = enc
        X_cat_list.append(X_c)

    if X_cat_list:
        X_cat = np.hstack(X_cat_list)
    else:
        X_cat = None

    # --- –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ---
    if X_num is not None and X_cat is not None:
        X = np.hstack([X_num, X_cat])
    elif X_num is not None:
        X = X_num
    else:
        X = X_cat

    return X.astype(np.float32)

#### FAISS –Ω–∞ —Å—ç–ø–º–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
def build_faiss_hnsw(X_S, M=32, efSearch=64):
    """
    –°–æ–∑–¥–∞—ë—Ç HNSW –∏–Ω–¥–µ–∫—Å FAISS –¥–ª—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ X_S

        X_S : —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –æ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∑–∏–∞—Ü–∏–∏
        M : —á–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π –≤ –≥—Ä–∞—Ñ–µ HNSW
        efSearch : —à–∏—Ä–∏–Ω–∞ –ø–æ–∏—Å–∫–∞
    output:
        index : faiss.IndexHNSWFlat
    """
    dim = X_S.shape[1]
    index = faiss.IndexHNSWFlat(dim, M)
    index.hnsw.efSearch = efSearch
    index.add(X_S)
    return index


###### –ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ –ª–µ–π–±–ª–æ–º —á–µ—Ä–µ–∑ FAISS HSNW
def assign_faiss(index, X_rest, labels_S, k=3, ood_threshold=None, weighted=True):
    """
    –ü—Ä–∏–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ—á–∫–∏ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –∏–∑ labels_S —á–µ—Ä–µ–∑ HNSW

        index : faiss.IndexHNSWFlat, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Å—ç–º–ø–ª–µ X_S
        X_rest : –Ω–æ–≤—ã–µ —Ç–æ—á–∫–∏ –∏–∑ –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
        labels_S : –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        k : —á–∏—Å–ª–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
        ood_threshold : float –∏–ª–∏ None, –ø–æ—Ä–æ–≥ –¥–ª—è OOD; –µ—Å–ª–∏ None - –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è
        –∫–∞–∫ 99-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –≤ S
        weighted : bool, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é

    output:
        assigned_labels : –ø—Ä–∏—Å–≤–æ–µ–Ω–Ω—ã–µ –ª–µ–π–±–ª
        assign_distance : —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        is_OOD : True –µ—Å–ª–∏ –æ–±—ä–µ–∫—Ç OOD
    """
    n_S = labels_S.shape[0]
    k = min(k, n_S)

    D, I = index.search(X_rest, k)  # distances & indices
    n_rest = X_rest.shape[0]
    assigned_labels = np.empty(n_rest, dtype=int)
    assign_distance = np.empty(n_rest, dtype=float)
    is_OOD = np.zeros(n_rest, dtype=bool)

    # OOD threshold –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω
    if ood_threshold is None:
        # —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π 99-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –≤—Å–µ—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ S
        # D_self: —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ 1-–≥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ –≤ S (exclude self)
        D_self, _ = index.search(index.reconstruct_n(0, n_S), 2)
        D_self = D_self[:, 1]  # –ø–µ—Ä–≤—ã–π —Å–æ—Å–µ–¥ = —Å–∞–º –æ–±—ä–µ–∫—Ç, –±–µ—Ä–µ–º 2-–π
        ood_threshold = np.quantile(D_self, 0.99)

    for i in range(n_rest):
        neigh_idxs = I[i]
        neigh_dists = D[i]
        neigh_labels = labels_S[neigh_idxs]

        if weighted:
            # –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ: 1/(dist+1e-9)
            weights = 1.0 / (neigh_dists + 1e-9)
            label_score = {}
            for lbl, w in zip(neigh_labels, weights):
                label_score[lbl] = label_score.get(lbl, 0.0) + w
            # –≤—ã–±–∏—Ä–∞–µ–º label —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—É–º–º—ã –≤–µ—Å–æ–≤
            assigned_label = max(label_score.items(), key=lambda x: x[1])[0]
        else:
            # —á–µ—Ä–µ–∑ ArgMax
            vals, counts = np.unique(neigh_labels, return_counts=True)
            assigned_label = vals[np.argmax(counts)]

        assigned_labels[i] = assigned_label
        # —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º label
        mask = neigh_labels == assigned_label
        assign_distance[i] = neigh_dists[mask].min()
        # OOD
        is_OOD[i] = assign_distance[i] > ood_threshold

    return assigned_labels, assign_distance, is_OOD

#################### –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ##################################

### –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
def analyze_all_clusters(df, target_col, num_cols, cat_cols, cluster_col="cluster", max_depth=6):
    results = {}
    df.reset_index(inplace=True, drop=True)
    if target_col!="No Target":
      # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
      if pd.api.types.is_numeric_dtype(df[target_col]):
        summary = df.groupby(cluster_col)[target_col].agg(
            ["count", "mean", "std", "min", "max", "median"]
        ).sort_values("mean", ascending=False)
        summary.columns = [
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤",
            "–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞",
            "–°—Ç–¥. –æ—Ç–∫–ª.",
            "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
            "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ",
            "–ú–µ–¥–∏–∞–Ω–∞"
        ]

      else:
        counts = (
            df.groupby([cluster_col, target_col])
            .size()
            .reset_index(name="count")
        )

        total = counts.groupby(cluster_col)["count"].transform("sum")
        counts["share"] = (counts["count"] / total * 100).round(2)

        # –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        mode_df = (
            counts.sort_values(["cluster", "count"], ascending=[True, False])
            .drop_duplicates(subset=[cluster_col])
            .rename(columns={target_col: "most_frequent"})
            .loc[:, [cluster_col, "most_frequent"]]
        )

        summary = (
            counts.pivot(index=cluster_col, columns=target_col, values="share")
            .fillna(0)
        )
        summary.columns = [f"{col} (% –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ)" for col in summary.columns]
        summary["–°–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è"] = mode_df.set_index(cluster_col)["most_frequent"]
        summary["–í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤"] = counts.groupby(cluster_col)["count"].sum().values
    
    else:
      summary = None

    # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ DecisionTreeClassifier
    X = df[num_cols + cat_cols]
    y = df[cluster_col].astype(str)

    preproc = ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(sparse_output=False, handle_unknown="ignore"), cat_cols)
        ]
    )

    clf = Pipeline([
        ("prep", preproc),
        ("tree", DecisionTreeClassifier(max_depth=max_depth, random_state=42))
    ])

    clf.fit(X, y)
    #### –°—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –æ–±—ä—è—Å–Ω—é—â–µ–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    super_tree = SuperTree(
        clf.named_steps["tree"],
        clf.named_steps["prep"].transform(X),
        y,
        clf.named_steps["prep"].get_feature_names_out(),
        np.unique(y))


    importances = pd.Series(
        clf.named_steps["tree"].feature_importances_,
        index=clf.named_steps["prep"].get_feature_names_out()
    ).sort_values(ascending=False)
    importances.name = "–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞"

    results["cluster_importances"] = importances

    return summary, super_tree, importances

######
### –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
def analyze_within_clusters(df, target_col, num_cols, cat_cols, cluster_col="cluster", cluster=0, max_depth=5):
    results = {}
    df.reset_index(inplace=True, drop=True)

    preproc = ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(sparse_output=False, handle_unknown="ignore"), cat_cols)
        ]
    )

    # –ê–Ω–∞–ª–∏–∑ —Ç–∞—Ä–≥–µ—Ç–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    target_results = {}
    subset = df[df[cluster_col] == cluster]

    X_sub = subset[num_cols + cat_cols]
    y_sub = subset[target_col]

    # –ï—Å–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —á–∏—Å–ª–æ–≤–æ–π DecisionTreeRegresor
    if pd.api.types.is_numeric_dtype(subset[target_col]) == True:

      model = Pipeline([
          ("prep", preproc),
          ("tree", DecisionTreeRegressor(max_depth=max_depth, random_state=42))
      ])

      model.fit(X_sub, y_sub)
      class_names = None

    # –ï—Å–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π DecisionTreeClassifier
    elif pd.api.types.is_object_dtype(subset[target_col]) == True:

      le = LabelEncoder()
      y_enc = le.fit_transform(y_sub)

      model = Pipeline([
          ("prep", preproc),
          ("tree", DecisionTreeClassifier(max_depth=max_depth, random_state=42))
      ])

      model.fit(X_sub, y_enc)
      class_names = le.classes_

    else:
        raise ValueError("–¢–∏–ø —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º")

    importances = pd.Series(
        model.named_steps["tree"].feature_importances_,
        index=model.named_steps["prep"].get_feature_names_out()
    ).sort_values(ascending=False)
    importances.name = "–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞"

    target_results[cluster] = importances

    st.subheader(f"\n –ö–ª–∞—Å—Ç–µ—Ä {cluster}: —Ñ–∞–∫—Ç–æ—Ä—ã –∏ –ø—Ä–∞–≤–∏–ª–∞, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ {target_col}")
    st.dataframe(importances.head(5))

    super_tree = SuperTree(
        model.named_steps["tree"],
        model.named_steps["prep"].transform(X_sub),
        y_sub.reset_index(drop=True),
        model.named_steps["prep"].get_feature_names_out(),
        target_names=class_names)

    return super_tree

######################## Streamlit layout ##################

st.set_page_config(page_title="–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", layout="wide")

st.title("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
st.write('''–î–∞–Ω–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
         (—á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ) —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º (–∏–ª–∏ –±–µ–∑ –Ω–µ–≥–æ) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HDBSCAN –∏–ª–∏
         Hierarchical agglomerative clustering, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–∞—è
         –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ Gower Distance. –ü–æ—Å–ª–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å
         –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –æ—Ü–µ–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.
         –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –ø–æ—á—Ç–∏ —É –≤—Å–µ—Ö –ø—É–Ω–∫—Ç–æ–≤ –µ—Å—Ç—å ‚ùî –ø—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π –≤—Å–ø–ª—ã–≤–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ 
         –∏ –¥–∞—Å—Ç –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ø–æ—è—Å–Ω–µ–Ω–∏–µ –ø–æ —Ñ—É–Ω–∫—Ü–∏–∏''')

st.header("1. –ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ===
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel —Ñ–∞–π–ª", type=["csv", "xlsx"])
with st.expander('''–ï—Å–ª–∏ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥–æ–π CSV –∏–ª–∏ —á–∏—Å–ª–æ–≤–æ–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å'''):
  option_decimal = st.radio(
      "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ—Å—è—Ç–∏—á–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å",
      (".", ","),
      horizontal=True)
  option_sep = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π —Ç–∏–ø —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –¥–ª—è CSV", value=",")

if uploaded_file is not None:
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏ —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª

    if uploaded_file.name.endswith(".csv"):
        if option_decimal == ".":
            option_thousand = ','
        else:
            option_thousand = None
        try:
          df = pd.read_csv(
              uploaded_file,
              sep=option_sep,
              decimal=option_decimal,
              thousands=option_thousand)
        except (ValueError, TypeError):
          st.error('–ü—Ä–æ–±–ª–µ–º–∞ –≤ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞', icon="üö®")
    else:
        df = pd.read_excel(uploaded_file)

    # === 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ===
    for col in df.select_dtypes(exclude=[np.number]).columns:
        try:
            df[col] = pd.to_datetime(df[col])
        except (ValueError, TypeError):
            pass  # –Ω–µ –¥–∞—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

    st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö —Ç–∏–ø—ã")
    st.dataframe(df.head())
    st.warning('''–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É—Å–ø–µ—à–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
                  –î–∞–ª–µ–µ –±—É–¥–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–≤–µ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É.''',
               icon="‚ö†Ô∏è")
    st.dataframe(df.dtypes.to_frame().T)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    st.subheader("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")

    # === 3. –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É ===

    help_to_num = '''–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ –≤–∞—à–µ–º—É –º–Ω–µ–Ω–∏—é –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º–∏,
                    –Ω–æ –æ–Ω–∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –∫–∞–∫ object —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∏–∑-–∑–∞ –Ω–∞–ª–∏—á–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö
                    —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏–ª–∏ –æ—à–∏–±–æ–∫'''

    to_num_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É",
                                 df.columns.tolist(),
                                 help=help_to_num
                                )
    df[to_num_cols] = df[to_num_cols].apply(pd.to_numeric, errors='coerce')

    # === 4. –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è ===

    help_drop = '''–í—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –Ω–µ–Ω—É–∂–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏'''

    drop_cols = st.multiselect('''–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è, –∏ –Ω–µ –∑–∞–±—É–¥—å—Ç–µ
                                  –ø—Ä–æ –∏–Ω–¥–µ–∫—Å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏''',
                               df.columns.tolist(),
                               help=help_drop
                              )
    if drop_cols:
        df = df.drop(columns=drop_cols)

    # === 3. –í—ã–±–æ—Ä —Ç–∞—Ä–≥–µ—Ç–∞ ===

    help_target = '''–î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ,
                    –Ω–æ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ–Ω –Ω–µ–æ–±—Ö–æ–¥–∏–º. –ï—Å–ª–∏ –≤—ã –Ω–µ —Ö–æ—Ç–∏—Ç–µ 
                    –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤—ã–±–µ—Ä–∏—Ç–µ –≤ –∫–æ–Ω—Ü–µ —Å–ø–∏—Å–∫–∞ No Target'''

    target_cols = df.columns.tolist()
    target_cols.append("No Target")
    target = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫",
        target_cols,
        help=help_target
        )

    if target!="No Target":
      if  df[target].nunique() > 1000 and pd.api.types.is_object_dtype(df[target]):
        st.error("–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —Å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é, –≤—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥–æ–π –ø—Ä–∏–∑–Ω–∞–∫")
      df.dropna(subset=[target], inplace=True)

    # === 5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è ===

    help_filter = '''–û—Ç–±–∏—Ä–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ,
                      –≤ —Ç–æ–º —á–∏—Å–ª–µ –∏ –ø–æ –¥–∞—Ç–∞–º'''

    filter_cols = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞",
        df.columns.tolist(),
        help=help_filter
    )
    if filter_cols:

      with st.expander("–û—Ç–∫—Ä—ã—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã"):
          for col in filter_cols:
              col_type = df[col].dtype

              if pd.api.types.is_datetime64_any_dtype(col_type):
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
                  min_date, max_date = df[col].min(), df[col].max()
                  start_date, end_date = st.date_input(
                      f"{col}: –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç",
                      [min_date, max_date],
                      key=col
                  )
                  mask = (df[col] >= pd.to_datetime(start_date)) & (df[col] <= pd.to_datetime(end_date))
                  df = df.loc[mask]

              elif pd.api.types.is_numeric_dtype(col_type):
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ —á–∏—Å–ª–æ–≤–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
                  min_val, max_val = float(df[col].min()), float(df[col].max())
                  val_range = st.slider(
                      f"{col}: –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π",
                      min_val, max_val, (min_val, max_val),
                      key=col
                  )
                  mask = (df[col] >= val_range[0]) & (df[col] <= val_range[1])
                  df = df.loc[mask]

              else:
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º/—Ç–µ–∫—Å—Ç—É
                  unique_vals = df[col].dropna().unique().tolist()
                  if len(unique_vals) > 1 and len(unique_vals) <= 100:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —Å–ø–∏—Å–∫–∏
                      selected_vals = st.multiselect(
                          f"{col}: –≤—ã–±–µ—Ä–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                          unique_vals,
                          default=unique_vals,
                          key=col
                      )
                      df = df[df[col].isin(selected_vals)]

    # === 5.1 –°–Ω–∏–∂–µ–Ω–∏–µ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ========
    if target!="No Target":
      cat_cols = (df
                  .drop(target, axis=1)
                  .select_dtypes(exclude=[np.number, np.datetime64])
                  .columns.tolist())
    else:
      cat_cols = (df
                  .select_dtypes(exclude=[np.number, np.datetime64])
                  .columns.tolist())

    help_card_cat = '''–î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
                      –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ø-10 –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ 'Other'''

    cat_cols_to_reduce = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
        cat_cols,
        help=help_card_cat
    )
    for col in cat_cols_to_reduce:
      top_cat = df[col].value_counts().head(10).index.tolist()
      df[col] = np.where(df[col].isin(top_cat), df[col], col+'_Other')

    # === 6. –†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ ===
    missing_option = st.radio("–ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ –¥–∞–Ω–Ω—ã—Ö?",
      ("–ó–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π –∏ MISSING", "–í—ã–±—Ä–æ—Å–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏"),
      horizontal=True)
    if target!="No Target":
      num_cols = (df
                  .drop(target, axis=1)
                  .select_dtypes(include=[np.number])
                  .columns.tolist())
      cat_cols = (df
                  .drop(target, axis=1)
                  .select_dtypes(exclude=[np.number, np.datetime64])
                  .columns.tolist())
    else:
      num_cols = (df
                  .select_dtypes(include=[np.number])
                  .columns.tolist())
      cat_cols = (df
                  .select_dtypes(exclude=[np.number, np.datetime64])
                  .columns.tolist())

    if missing_option == "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π –∏ MISSING":
      # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π, –ª–∏–±–æ __MISSING__
      for col in num_cols:
          df[col] = df[col].fillna(df[col].median())
      for col in cat_cols:
          df[col] = df[col].fillna("__MISSING__").astype(str)
    else:
      # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
      df = df.dropna()

    # === 7. –†–µ–∑—É–ª—å—Ç–∞—Ç ===
    st.subheader("–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

    st.warning('''–£ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –±–æ–ª—å—à–µ 100 —Ä–µ–¥–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                  –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞—é—Ç—Å—è –Ω–∞ 'Other'!''')
    for col in cat_cols:
      if df[col].nunique() > 100:
        top_cat = df[col].value_counts().head(20).index.tolist()
        df[col] = np.where(df[col].isin(top_cat), df[col], col+'_Other')

    df = df.reset_index(drop=True)
    st.write(f"–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è {len(df.head(500))} –∏–∑ {len(df)} —Å—Ç—Ä–æ–∫")
    st.dataframe(df.head(500))
    st.info("–ï—â–µ —Ä–∞–∑ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    st.dataframe(df.dtypes.to_frame().T)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    datetime_columns = df.select_dtypes(include=[np.datetime64]).columns
    if target!="No Target":
      y = df[target]
      X = df.drop(target, axis=1)
    else:
      X = df.copy()
      y = None
    X = X.drop(datetime_columns, axis=1)

    ############# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ################################
    st.header("2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
    st.write('''–î–∞–Ω–Ω–∞—è —á–∞—Å—Ç—å —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ 2 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç—è–∂–µ–ª—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —ç—Ç–∞–ø–∞.
                –°–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π, –¥–∞–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–µ –≤
                –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö.
                –ü—Ä–∏ —Å–º–µ–Ω–µ –º–µ—Ç–æ–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –º–µ—Ç—Ä–∏–∫—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ.''')
    ### –°—á–∏—Ç–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ì–æ–≤–µ—Ä–∞
    help_gower = '''–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ì–∞—É—ç—Ä–∞ ‚Äî —ç—Ç–æ –º–µ—Ä–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
                  –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É
                  —Ç–æ—á–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —á–∏—Å–ª–æ–≤—ã—Ö, –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ –ø–æ—Ä—è–¥–∫–æ–≤—ã—Ö
                  –∞—Ç—Ä–∏–±—É—Ç–æ–≤. –ú–µ—Ç–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—É—Ç—ë–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ —Å—Ç–µ–ø–µ–Ω–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∫–∞–∂–¥–æ–≥–æ
                  –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1 —Å –ø–æ—Å–ª–µ–¥—É—é—â–∏–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ
                  –∑–Ω–∞—á–µ–Ω–∏—è —ç—Ç–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.'''

    st.subheader("–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (Gower Distance)", help=help_gower)
    st.write('''–î–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
                –º–µ–∂–¥—É –æ–±—ä–µ–∫—Ç–∞–º–∏. –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–∞—Å—á–µ—Ç –±–µ–∑ —É—á–µ—Ç–∞ –≤–µ—Å–æ–≤.''')

    help_weights = '''–ê–ª–≥–æ—Ä–∏—Ç–º –º–æ–∂–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏
                      –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –≠—Ç–æ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏,
                      –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –≤–∫–ª–∞–¥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                      (–Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö). –û–Ω –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å
                      –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º—É–ª—ã, –∫–æ—Ç–æ—Ä–∞—è
                      –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞ ‚Äã‚Äã–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –∏–ª–∏
                      –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö.'''

    option_weights = st.radio("–†–∞—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º?",
                              ("–ù–µ—Ç", "–î–∞"),
                              horizontal=True,
                              help=help_weights)

    if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É"):
      # –ì–æ—Ç–æ–≤–∏–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è FAISS, –Ω–∞ —Å–ª—É—á–∞–π, –Ω–µ—Å–ª–∏ –≤—ã–±–æ—Ä–∫–∞ –±—É–¥–µ—Ç –±–æ–ª—å—à–µ 8000
      N = X.shape[0]
      S = min(8000, N)
      idx_all = np.arange(N)
      idx_S = np.random.choice(idx_all, size=S, replace=False)
      idx_rest = np.setdiff1d(idx_all, idx_S)
      st.session_state.idx_S = idx_S
      st.session_state.idx_rest = idx_rest
      # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      if option_weights == "–î–∞" and target!="No Target":
        weights = compute_feature_weights(X.iloc[idx_S], y.iloc[idx_S], num_cols, cat_cols)
      elif option_weights == "–î–∞" and target=="No Target":
        st.error('''–í—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤ —Ä–∞—Å—á–µ—Ç–∞—Ö! –í–µ—Å–∞ –Ω–µ –±—É–¥—É—Ç 
                    —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫–∏!''')
        weights = None
      else:
        weights = None

      # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      st.session_state.D = compute_gower_matrix(
          X.iloc[idx_S],
          num_cols,
          cat_cols,
          numeric_ranges=None,
          weights=weights
      )
      st.success("–ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞")

    #### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º
    st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏")

    help_cluster = '''1.HDBSCAN (–º–µ–¥–ª–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç) ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏,
                      –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö. –û–Ω –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã,
                      —Å—Ç—Ä–æ—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ—Å—Ç–æ–≤–Ω–æ–µ –¥–µ—Ä–µ–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞
                      —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–æ–±—Ä–∞–∑—É—è –µ–≥–æ –≤ –∏–µ—Ä–∞—Ä—Ö–∏—é –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –∞ –∑–∞—Ç–µ–º
                      –≤—ã–±–∏—Ä–∞—è –∏–∑ –Ω–µ—ë —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Ö —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
                      –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏. –õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–º–∞—Ç—Ä—ã –ø–æ–¥–±–∏—Ä–∞—é—Ç—Å—è
                      –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –æ—Ü–µ–Ω–∫–æ–π —á–µ—Ä–µ–∑ Validity Index.
                      2.HAC (–±—ã—Å—Ç—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç) - –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∞–≥–ª–æ–º–µ—Ä–∞—Ç–∏–≤–Ω–∞—è
                      –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è. –ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ¬´—Å–Ω–∏–∑—É –≤–≤–µ—Ä—Ö¬ª, –∫–æ—Ç–æ—Ä—ã–π
                      –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç–æ–≥–æ, —á—Ç–æ –∫–∞–∂–¥–∞—è —Ç–æ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π
                      –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä, –∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –±–ª–∏–∂–∞–π—à–∏–µ –ø–∞—Ä—ã
                      –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –ø–æ–∫–∞ –Ω–µ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
                      –ø–æ–ª—É—á–∞–µ—Ç—Å—è –¥—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –Ω–∞–∑—ã–≤–∞–µ–º–∞—è –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–æ–π,
                      –∫–æ—Ç–æ—Ä–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é —Å–ª–∏—è–Ω–∏–π. –õ—É—á—à–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                      –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –æ—Ü–µ–Ω–∫–æ–π —á–µ—Ä–µ–∑ Silhoutte score'''

    option_method = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
                             ("HDBSCAN", "HAC"),
                             horizontal=True,
                             help=help_cluster)

    if st.button("–ù–∞—á–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"):
      if hasattr(st.session_state, "D"):
        st.session_state.labels = clusterize(
            X.iloc[st.session_state.idx_S],
            st.session_state.D,
            method=option_method
        )
      else:
        st.error("–†–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Ä–∞—Å—Å—Ç–æ–Ω–Ω–∏—è –ì–æ–≤–µ—Ä–∞")

      # --- FAISS HNSW index –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ > 8000 ---
      if X.shape[0] > 8000:
        st.subheader("FAISS HNSW —Ä–∞–∑–º–µ—Ç–∫–∞ –ª–µ–π–±–ª–æ–≤")
        X_vectorized = vectorize(X, num_cols, cat_cols)
        X_S = X_vectorized[st.session_state.idx_S]
        X_rest = X_vectorized[st.session_state.idx_rest]
        index = build_faiss_hnsw(X_S, M=32, efSearch=64)

        # --- assign –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ---
        labels_rest, dist_rest, is_ood = assign_faiss(index,
                                                      X_rest,
                                                      st.session_state.labels,
                                                      k=3
                                                      )

        # --- —Å–æ–±—Ä–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
        result = df.copy()
        result['cluster'] = np.nan
        result['assign_distance'] = np.nan
        result['is_OOD'] = False

        # –º–µ—Ç–∫–∏ –¥–ª—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏
        result.loc[st.session_state.idx_S, 'cluster'] = st.session_state.labels
        result.loc[st.session_state.idx_S, 'assign_distance'] = 0.0
        result.loc[st.session_state.idx_S, 'is_OOD'] = False

        # –º–µ—Ç–∫–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        result.loc[st.session_state.idx_rest, 'cluster'] = labels_rest
        result.loc[st.session_state.idx_rest, 'assign_distance'] = dist_rest
        result.loc[st.session_state.idx_rest, 'is_OOD'] = is_ood
        st.session_state.result = result
        st.success(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º {option_method} —Å –¥–æ—Ä–∞–∑–º–µ—Ç–∫–æ–π FAISS –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

      else:
        result = df.copy()
        result.loc[st.session_state.idx_S, 'cluster'] = st.session_state.labels
        st.session_state.result = result
        st.success(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º {option_method} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    ##### –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    if hasattr(st.session_state, "result"):
      st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç")
      st.write(f"–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è {len(st.session_state.result.head(500))}",
               f"–∏–∑ {len(st.session_state.result)} —Å—Ç—Ä–æ–∫")
      st.dataframe(st.session_state.result.head(500))

      st.download_button(
            label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ CSV",
            data=st.session_state.result.to_csv(index=False).encode('utf-8'),
            file_name="result.csv",
            mime="text/csv",
        )

      help_analyze = '''–ê–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é —Ä–µ—à–∞—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤
                        –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ/–∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏'''

      st.header("3. –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", help=help_analyze)
      if st.button("–ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑", key="1"):
        st.session_state.summary, st.session_state.super_tree, st.session_state.importances = (
            analyze_all_clusters(
                st.session_state.result,
                target,
                num_cols,
                cat_cols
                )
        )

        with NamedTemporaryFile(suffix=".html", delete=False) as f:
          st.session_state.super_tree.save_html(f.name)
          st.session_state.super_tree_html = open(f.name, "r", encoding="utf-8").read()

      if hasattr(st.session_state, "super_tree"):

        help_overlclust = '''–°–º–æ—Ç—Ä–∏–º –∫–∞–∫ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º,
                             –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç –∫–ª–∞—Å—Ç–µ—Ä'''

        st.subheader("–û–±—â–µ–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑", help=help_overlclust)
        st.text(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ {target} –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
        st.dataframe(st.session_state.summary)
        st.text("\n –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —Ñ–æ—Ä–º–∏—Ä—É—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:")
        st.dataframe(st.session_state.importances.head(10))
        st.text("–ü—Ä–∞–≤–∏–ª–∞, –æ–±—ä—è—Å–Ω—è—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")
        if "super_tree_html" in st.session_state:
          html(st.session_state.super_tree_html, height=650)

      if target!="No Target":
        help_intraclust = '''–°–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–ª–∏—è—é—Ç –Ω–∞ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫
                            –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞'''

        st.subheader("–í–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑", help=help_intraclust)
        option_cluster = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
            st.session_state.result['cluster'].unique()
        )
        if st.button("–ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑", key="2"):
          cluster_tree = analyze_within_clusters(
              st.session_state.result,
              target,
              num_cols,
              cat_cols,
              cluster=option_cluster
          )
          with NamedTemporaryFile(suffix=".html", delete=False) as f1:
            cluster_tree.save_html(f1.name)
            html(f1.read(), height=650)

else:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")