import pandas as pd
import numpy as np
import time
import random
import streamlit as st

# plots
import seaborn as sns
import matplotlib.pyplot as plt

# HAC
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score

# HDBSCAN
import hdbscan
from hdbscan.validity import validity_index

# FeatureEncode
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import LabelEncoder
#from category_encoders import CatBoostEncoder

# Feature weights
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression

# Cluster Explanation
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree

from tempfile import NamedTemporaryFile
from supertree import SuperTree
from streamlit.components.v1 import html
import faiss

# –†–∞—Å—á–µ—Ç —Ä–µ–Ω–∂–µ–π –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_numeric_ranges(df, num_cols, method='minmax'):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {col: range} –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      - 'minmax'  : range = max - min
      - 'iqr' : range = (Q3-Q1)
    """
    ranges = {}
    for c in num_cols:
        col = df[c].dropna().to_numpy(dtype=float)
        if col.size == 0:
            ranges[c] = 1.0
            continue

        if method == 'minmax':
            r = float(np.max(col) - np.min(col))
        elif method == 'iqr':
            q1 = np.percentile(col, 25)
            q3 = np.percentile(col, 75)
            r = float((q3 - q1))
        else:
            raise ValueError("unknown method")

        # –ï—Å–ª–∏ range —Å–ª–∏—à–∫–æ–º –º–∞–ª, —Ç–æ —Å—Ç–∞–≤–∏–º –µ–≥–æ 1.0
        if not np.isfinite(r) or r <= 1e-8:
            r = 1.0
        ranges[c] = r
    return ranges

# –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_feature_weights(df, target, num_cols, cat_cols):
    """
    –°—á–∏—Ç–∞–µ—Ç –≤–µ—Å–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (MI) —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
    df : –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    target : pd.Series –∏–ª–∏ np.array —Ç–∞—Ä–≥–µ—Ç (—á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π)

    output:
    weights : dict
        {column_name: weight}, –≤–µ—Å–∞ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã (—Å—É–º–º–∞ = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).
    """
    y = target.values

    # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    df_enc = df.copy()
    discrete_features = []
    for c in cat_cols:
        le = LabelEncoder()
        df_enc[c] = le.fit_transform(df_enc[c].astype(str))
        discrete_features.append(df.columns.get_loc(c))  # –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    X = df_enc.to_numpy()

    # –í—ã—á–∏—Å–ª—è–µ–º MI
    if pd.api.types.is_numeric_dtype(target) == True:
        mi = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=42)
    elif pd.api.types.is_object_dtype(target) == True:
        mi = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=42)
    else:
        raise ValueError("–¢–∏–ø —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º")

    # –ù–æ—Ä–º–∏—Ä—É–µ–º –≤–µ—Å–∞ (—Å—É–º–º–∞ = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    mi = np.maximum(mi, 1e-9)  # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –Ω—É–ª–µ–≤—ã—Ö
    mi_normalized = mi / mi.sum() * len(mi)

    weights = {col: w for col, w in zip(df.columns, mi_normalized)}
    return weights

# –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_gower_matrix(df, num_cols, cat_cols, numeric_ranges=None,
                          weights=None, dtype=np.float32):
    """
    - numeric_ranges: dict col->range. –ï—Å–ª–∏ None, computed by minmax on df.
    - weights: dict col->weight. –ï—Å–ª–∏ None - –≤—Å–µ 1
    """
    X = df.reset_index(drop=True)
    n = len(X)
    if n == 0:
        return np.zeros((0,0), dtype=dtype)

    if numeric_ranges is None:
        numeric_ranges = compute_numeric_ranges(X, num_cols, method='minmax')

    D = np.zeros((n, n), dtype=np.float64)  # accumulate in float64 for numeric stability

    # weights
    if weights is None:
        # equal weight per feature
        w_num = {c: 1.0 for c in num_cols}
        w_cat = {c: 1.0 for c in cat_cols}
    else:
        # user-provided; missing keys default to 1.0
        w_num = {c: float(weights.get(c, 1.0)) for c in num_cols}
        w_cat = {c: float(weights.get(c, 1.0)) for c in cat_cols}

    # numeric part
    for c in num_cols:
        col = X[c].to_numpy(dtype=float)
        rng = numeric_ranges.get(c, 1.0)
        # normalized differences (broadcast)
        mat = np.abs(col[:, None] - col[None, :]) / rng
        D += w_num.get(c, 1.0) * mat

    # categorical part
    for c in cat_cols:
        col = X[c].astype(str).to_numpy()
        neq = (col[:, None] != col[None, :]).astype(np.float64)
        D += w_cat.get(c, 1.0) * neq

    # normalize by total weights sum
    total_weight = sum(w_num.values()) + sum(w_cat.values())
    if total_weight <= 0:
        total_weight = 1.0
    D = (D / float(total_weight)).astype(dtype)
    return D.astype(np.float64)

### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
def clusterize(df, D, method="HDBSCAN", max_k=20):
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ.
    method = "hdbscan" –∏–ª–∏ "hac"
    """

    if method == "HDBSCAN":
      best_score = -1
      best_params = None
      best_labels = None
      d = df.shape[1]
      # specify parameters and distributions to sample from
      param_grid = [
          {"min_cluster_size": mcs, "min_samples": ms}
          for mcs in [10, 20, 30, 50]
          for ms in [10, 20, 30, 50]
      ]

      for params in param_grid:
        cl = hdbscan.HDBSCAN(metric="precomputed", **params)
        labels = cl.fit_predict(D)

        if len(set(labels)) > 1:
            score = validity_index(
                D,
                labels,
                metric="precomputed",
                d=d
            )
            if score > best_score:
                best_score = score
                best_params = params
                best_labels = labels

    elif method == "HAC":
      # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤ condensed form
      condensed = squareform(D, checks=False)

      # linkage (average linkage –ª—É—á—à–µ –¥–ª—è Gower)
      Z = linkage(condensed, method="complete")

      # –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k –ø–æ silhouette
      best_k, best_score, best_labels = None, -1, None
      for k in range(2, min(max_k, len(df)//10)):
          labels = fcluster(Z, k, criterion="maxclust")
          if len(np.unique(labels)) < 2:
              continue
          try:
              score = silhouette_score(D, labels, metric="precomputed")
          except Exception:
              continue
          if score > best_score:
              best_score, best_k, best_labels = score, k, labels

      if best_labels is None:
          # fallback: –≤—Å—ë –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
          best_labels = np.ones(len(df), dtype=int)

      labels = best_labels

    else:
      raise ValueError("method must be 'HDBSCAN' or 'HAC'")

    return labels

### –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAISS
def vectorize(df, num_cols, cat_cols, ohe_thresh=20, hash_dim=32):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –≤ —á–∏—Å–ª–æ–≤–æ–π –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ / –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–µ–π.

    Parameters:
        df : pd.DataFrame
        numeric_cols : list[str] - —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        cat_cols : list[str] - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        ohe_thresh : int - –º–∞–∫—Å —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è OHE
        hash_dim : int - —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è FeatureHasher

    output:
        X : np.ndarray, float32
    """
    # --- —á–∏—Å–ª–æ–≤—ã–µ ---
    scaler = RobustScaler()
    X_num = scaler.fit_transform(df[num_cols]) if num_cols else None

    # --- –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ---
    X_cat_list = []
    encoders = {}

    for c in cat_cols:
        n_unique = df[c].nunique()
        col_data = df[[c]].astype(str)

        if n_unique <= ohe_thresh:
            # OHE
            enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            X_c = enc.fit_transform(col_data)
            encoders[c] = enc
        else:
            # FeatureHasher
            enc = FeatureHasher(n_features=hash_dim, input_type='string')
            # FeatureHasher –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: [{cat_val: 1}, ...]
            X_c = enc.transform([{val: 1} for val in col_data[c]]).toarray()
            encoders[c] = enc
        X_cat_list.append(X_c)

    if X_cat_list:
        X_cat = np.hstack(X_cat_list)
    else:
        X_cat = None

    # --- –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ---
    if X_num is not None and X_cat is not None:
        X = np.hstack([X_num, X_cat])
    elif X_num is not None:
        X = X_num
    else:
        X = X_cat

    return X.astype(np.float32)

#### FAISS –Ω–∞ —Å—ç–ø–º–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ
def build_faiss_hnsw(X_S, M=32, efSearch=64):
    """
    –°–æ–∑–¥–∞—ë—Ç HNSW –∏–Ω–¥–µ–∫—Å FAISS –¥–ª—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ X_S

        X_S : —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –æ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∑–∏–∞—Ü–∏–∏
        M : —á–∏—Å–ª–æ —Å–æ—Å–µ–¥–µ–π –≤ –≥—Ä–∞—Ñ–µ HNSW
        efSearch : —à–∏—Ä–∏–Ω–∞ –ø–æ–∏—Å–∫–∞
    output:
        index : faiss.IndexHNSWFlat
    """
    dim = X_S.shape[1]
    index = faiss.IndexHNSWFlat(dim, M)
    index.hnsw.efSearch = efSearch
    index.add(X_S)
    return index


###### –ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ –ª–µ–π–±–ª–æ–º —á–µ—Ä–µ–∑ FAISS HSNW
def assign_faiss(index, X_rest, labels_S, k=3, ood_threshold=None, weighted=True):
    """
    –ü—Ä–∏–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ—á–∫–∏ –∫ –∫–ª–∞—Å—Ç–µ—Ä–∞–º –∏–∑ labels_S —á–µ—Ä–µ–∑ HNSW

        index : faiss.IndexHNSWFlat, –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Å—ç–º–ø–ª–µ X_S
        X_rest : –Ω–æ–≤—ã–µ —Ç–æ—á–∫–∏ –∏–∑ –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
        labels_S : –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        k : —á–∏—Å–ª–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
        ood_threshold : float –∏–ª–∏ None, –ø–æ—Ä–æ–≥ –¥–ª—è OOD; –µ—Å–ª–∏ None - –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–∞–∫ 99-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –≤ S
        weighted : bool, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é

    output:
        assigned_labels : –ø—Ä–∏—Å–≤–æ–µ–Ω–Ω—ã–µ –ª–µ–π–±–ª
        assign_distance : —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        is_OOD : True –µ—Å–ª–∏ –æ–±—ä–µ–∫—Ç OOD
    """
    n_S = labels_S.shape[0]
    k = min(k, n_S)

    D, I = index.search(X_rest, k)  # distances & indices
    n_rest = X_rest.shape[0]
    assigned_labels = np.empty(n_rest, dtype=int)
    assign_distance = np.empty(n_rest, dtype=float)
    is_OOD = np.zeros(n_rest, dtype=bool)

    # OOD threshold –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω
    if ood_threshold is None:
        # —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π 99-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –≤—Å–µ—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ S
        # D_self: —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ 1-–≥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ –≤ S (exclude self)
        D_self, _ = index.search(index.reconstruct_n(0, n_S), 2)
        D_self = D_self[:, 1]  # –ø–µ—Ä–≤—ã–π —Å–æ—Å–µ–¥ = —Å–∞–º –æ–±—ä–µ–∫—Ç, –±–µ—Ä–µ–º 2-–π
        ood_threshold = np.quantile(D_self, 0.99)

    for i in range(n_rest):
        neigh_idxs = I[i]
        neigh_dists = D[i]
        neigh_labels = labels_S[neigh_idxs]

        if weighted:
            # –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ: 1/(dist+1e-9)
            weights = 1.0 / (neigh_dists + 1e-9)
            label_score = {}
            for lbl, w in zip(neigh_labels, weights):
                label_score[lbl] = label_score.get(lbl, 0.0) + w
            # –≤—ã–±–∏—Ä–∞–µ–º label —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—É–º–º—ã –≤–µ—Å–æ–≤
            assigned_label = max(label_score.items(), key=lambda x: x[1])[0]
        else:
            # —á–µ—Ä–µ–∑ ArgMax
            vals, counts = np.unique(neigh_labels, return_counts=True)
            assigned_label = vals[np.argmax(counts)]

        assigned_labels[i] = assigned_label
        # —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º label
        mask = neigh_labels == assigned_label
        assign_distance[i] = neigh_dists[mask].min()
        # OOD
        is_OOD[i] = assign_distance[i] > ood_threshold

    return assigned_labels, assign_distance, is_OOD

#################### –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ ##################################

### –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
def analyze_all_clusters(df, target_col, num_cols, cat_cols, cluster_col="cluster", max_depth=6):
    results = {}
    df.reset_index(inplace=True, drop=True)
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    if pd.api.types.is_numeric_dtype(df[target_col]):
      summary = df.groupby(cluster_col)[target_col].agg(
          ["count", "mean", "std", "min", "max", "median"]
      ).sort_values("mean", ascending=False)
      summary.columns = [
          "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤",
          "–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞",
          "–°—Ç–¥. –æ—Ç–∫–ª.",
          "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ",
          "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ",
          "–ú–µ–¥–∏–∞–Ω–∞"
      ]

    else:
      counts = (
          df.groupby([cluster_col, target_col])
          .size()
          .reset_index(name="count")
      )

      total = counts.groupby(cluster_col)["count"].transform("sum")
      counts["share"] = (counts["count"] / total * 100).round(2)

      # –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
      mode_df = (
          counts.sort_values(["cluster", "count"], ascending=[True, False])
          .drop_duplicates(subset=[cluster_col])
          .rename(columns={target_col: "most_frequent"})
          .loc[:, [cluster_col, "most_frequent"]]
      )

      summary = (
          counts.pivot(index=cluster_col, columns=target_col, values="share")
          .fillna(0)
      )
      summary.columns = [f"{col} (% –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ)" for col in summary.columns]
      summary["–°–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è"] = mode_df.set_index(cluster_col)["most_frequent"]
      summary["–í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤"] = counts.groupby(cluster_col)["count"].sum().values

    # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ DecisionTreeClassifier
    X = df[num_cols + cat_cols]
    y = df[cluster_col].astype(str)

    preproc = ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(sparse_output=False, handle_unknown="ignore"), cat_cols)
        ]
    )

    clf = Pipeline([
        ("prep", preproc),
        ("tree", DecisionTreeClassifier(max_depth=max_depth, random_state=42))
    ])

    clf.fit(X, y)
    #### –°—Ç—Ä–æ–∏–º –¥–µ—Ä–µ–≤–æ –æ–±—ä—è—Å–Ω—é—â–µ–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    super_tree = SuperTree(
        clf.named_steps["tree"],
        clf.named_steps["prep"].transform(X),
        y,
        clf.named_steps["prep"].get_feature_names_out(),
        np.unique(y))


    importances = pd.Series(
        clf.named_steps["tree"].feature_importances_,
        index=clf.named_steps["prep"].get_feature_names_out()
    ).sort_values(ascending=False)
    importances.name = "–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞"

    results["cluster_importances"] = importances

    return summary, super_tree, importances

######
### –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
def analyze_within_clusters(df, target_col, num_cols, cat_cols, cluster_col="cluster", cluster=0, max_depth=5):
    results = {}
    df.reset_index(inplace=True, drop=True)

    preproc = ColumnTransformer(
        transformers=[
            ("num", "passthrough", num_cols),
            ("cat", OneHotEncoder(sparse_output=False, handle_unknown="ignore"), cat_cols)
        ]
    )

    st.header("–í–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    # –ê–Ω–∞–ª–∏–∑ —Ç–∞—Ä–≥–µ—Ç–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    target_results = {}
    subset = df[df[cluster_col] == cluster]

    X_sub = subset[num_cols + cat_cols]
    y_sub = subset[target_col]

    # –ï—Å–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —á–∏—Å–ª–æ–≤–æ–π DecisionTreeRegresor
    if pd.api.types.is_numeric_dtype(subset[target_col]) == True:

      model = Pipeline([
          ("prep", preproc),
          ("tree", DecisionTreeRegressor(max_depth=max_depth, random_state=42))
      ])

      model.fit(X_sub, y_sub)
      class_names = None

    # –ï—Å–ª–∏ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π DecisionTreeClassifier
    elif pd.api.types.is_object_dtype(subset[target_col]) == True:

      le = LabelEncoder()
      y_enc = le.fit_transform(y_sub)

      model = Pipeline([
          ("prep", preproc),
          ("tree", DecisionTreeClassifier(max_depth=max_depth, random_state=42))
      ])

      model.fit(X_sub, y_enc)
      class_names = le.classes_

    else:
        raise ValueError("–¢–∏–ø —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–≤—ã–º –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º")

    importances = pd.Series(
        model.named_steps["tree"].feature_importances_,
        index=model.named_steps["prep"].get_feature_names_out()
    ).sort_values(ascending=False)
    importances.name = "–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞"

    target_results[cluster] = importances

    st.subheader(f"\n –ö–ª–∞—Å—Ç–µ—Ä {cluster}: —Ñ–∞–∫—Ç–æ—Ä—ã –∏ –ø—Ä–∞–≤–∏–ª–∞, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ —Ç–∞—Ä–≥–µ—Ç")
    st.dataframe(importances.head(5))

    super_tree = SuperTree(
        model.named_steps["tree"],
        model.named_steps["prep"].transform(X_sub),
        y_sub.reset_index(drop=True),
        model.named_steps["prep"].get_feature_names_out(),
        target_names=class_names)

    return super_tree

######################## Streamlit layout ##################

st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö", layout="wide")

st.title("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ===
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel —Ñ–∞–π–ª", type=["csv", "xlsx"])
with st.expander("–ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º"):
  option_decimal = st.radio(
      "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ—Å—è—Ç–∏—á–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å",
      (".", ","),
      horizontal=True)
  option_sep = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π —Ç–∏–ø —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –¥–ª—è CSV", value=",")

if uploaded_file is not None:
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏ —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª

    if uploaded_file.name.endswith(".csv"):
        if option_decimal == ".":
            option_thousand = ','
        else:
            option_thousand = None
        try:
          df = pd.read_csv(
              uploaded_file,
              sep=option_sep,
              decimal=option_decimal,
              thousands=option_thousand)
        except (ValueError, TypeError):
          st.error('–ü—Ä–æ–±–ª–µ–º–∞ –≤ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞', icon="üö®")
    else:
        df = pd.read_excel(uploaded_file)

    # === 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ===
    for col in df.select_dtypes(exclude=[np.number]).columns:
        try:
            df[col] = pd.to_datetime(df[col])
        except (ValueError, TypeError):
            pass  # –Ω–µ –¥–∞—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

    st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö —Ç–∏–ø—ã")
    st.dataframe(df.head())
    st.write("–í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    st.dataframe(df.dtypes.to_frame().T)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    st.subheader("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")

    # === 3. –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É ===
    to_num_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ —á–∏—Å–ª–æ–≤–æ–º—É —Ç–∏–ø—É",
                                df.columns.tolist())    
    df[to_num_cols] = df[to_num_cols].apply(pd.to_numeric, errors='coerce')

    # === 4. –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è ===
    drop_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è, –∏ –Ω–µ –∑–∞–±—É–¥—å—Ç–µ –ø—Ä–æ –∏–Ω–¥–µ–∫—Å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏",
                               df.columns.tolist())
    if drop_cols:
        df = df.drop(columns=drop_cols)

    # === 3. –í—ã–±–æ—Ä —Ç–∞—Ä–≥–µ—Ç–∞ ===
    target = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫", df.columns.tolist())
    if  df[target].nunique() > 1000 and pd.api.types.is_object_dtype(df[target]):
      st.error("–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ —Å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é, –≤—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥–æ–π –ø—Ä–∏–∑–Ω–∞–∫")
    df.dropna(subset=[target], inplace=True)

    # === 5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è ===
    filter_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞", df.columns.tolist())
    if filter_cols:

      with st.expander("–û—Ç–∫—Ä—ã—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã"):
          for col in filter_cols:
              col_type = df[col].dtype

              if pd.api.types.is_datetime64_any_dtype(col_type):
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
                  min_date, max_date = df[col].min(), df[col].max()
                  start_date, end_date = st.date_input(
                      f"{col}: –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç",
                      [min_date, max_date],
                      key=col
                  )
                  mask = (df[col] >= pd.to_datetime(start_date)) & (df[col] <= pd.to_datetime(end_date))
                  df = df.loc[mask]

              elif pd.api.types.is_numeric_dtype(col_type):
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ —á–∏—Å–ª–æ–≤–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
                  min_val, max_val = float(df[col].min()), float(df[col].max())
                  val_range = st.slider(
                      f"{col}: –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π",
                      min_val, max_val, (min_val, max_val),
                      key=col
                  )
                  mask = (df[col] >= val_range[0]) & (df[col] <= val_range[1])
                  df = df.loc[mask]

              else:
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º/—Ç–µ–∫—Å—Ç—É
                  unique_vals = df[col].dropna().unique().tolist()
                  if len(unique_vals) > 1 and len(unique_vals) <= 100:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —Å–ø–∏—Å–∫–∏
                      selected_vals = st.multiselect(
                          f"{col}: –≤—ã–±–µ—Ä–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                          unique_vals,
                          default=unique_vals,
                          key=col
                      )
                      df = df[df[col].isin(selected_vals)]

    # === 6. –†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ ===
    missing_option = st.radio("–ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ –¥–∞–Ω–Ω—ã—Ö?",
      ("–ó–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π –∏ MISSING", "–í—ã–±—Ä–æ—Å–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏"),
      horizontal=True)
    num_cols = (df
                .drop(target, axis=1)
                .select_dtypes(include=[np.number])
                .columns.tolist())
    cat_cols = (df
                .drop(target, axis=1)
                .select_dtypes(exclude=[np.number, np.datetime64])
                .columns.tolist())

    if missing_option == "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π –∏ MISSING":
      # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π, –ª–∏–±–æ __MISSING__
      for col in num_cols:
          df[col] = df[col].fillna(df[col].median())
      for col in cat_cols:
          df[col] = df[col].fillna("__MISSING__").astype(str)
    else:
      # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
      df = df.dropna()

    # === 7. –†–µ–∑—É–ª—å—Ç–∞—Ç ===
    st.subheader("–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")

    st.write("–£ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –±–æ–ª—å—à–µ 100 —Ä–µ–¥–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞—é—Ç—Å—è –Ω–∞ 'Other'!")
    for col in cat_cols:
      if df[col].nunique() > 100:
        top_cat = df[col].value_counts().head(20).index.tolist()
        df[col] = np.where(df[col].isin(top_cat), df[col], 'Other')

    df = df.reset_index(drop=True)
    st.write(f"–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è {len(df.head(500))} –∏–∑ {len(df)} —Å—Ç—Ä–æ–∫")
    st.dataframe(df.head(500))
    st.write("–ï—â–µ —Ä–∞–∑ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    st.dataframe(df.dtypes.to_frame().T)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    datetime_columns = df.select_dtypes(include=[np.datetime64]).columns
    X = df.drop(target, axis=1)
    X = X.drop(datetime_columns, axis=1)
    y = df[target]

    ############# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ################################
    st.header("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
    ### –°—á–∏—Ç–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ì–æ–≤–µ—Ä–∞
    st.subheader("–†–∞—Å—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞")

    option_weights = st.radio("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º?",
                              ("–ù–µ—Ç", "–î–∞"),
                              horizontal=True)

    if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞"):
      # –ì–æ—Ç–æ–≤–∏–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è FAISS, –Ω–∞ —Å–ª—É—á–∞–π, –Ω–µ—Å–ª–∏ –≤—ã–±–æ—Ä–∫–∞ –±—É–¥–µ—Ç –±–æ–ª—å—à–µ 8000
      N = X.shape[0]
      S = min(8000, N)
      idx_all = np.arange(N)
      idx_S = np.random.choice(idx_all, size=S, replace=False)
      idx_rest = np.setdiff1d(idx_all, idx_S)
      st.session_state.idx_S = idx_S
      st.session_state.idx_rest = idx_rest
      # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      if option_weights == "–î–∞":
        weights = compute_feature_weights(X.iloc[idx_S], y.iloc[idx_S], num_cols, cat_cols)
      else:
        weights = None

      # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      st.session_state.D = compute_gower_matrix(
          X.iloc[idx_S],
          num_cols,
          cat_cols,
          numeric_ranges=None,
          weights=weights
      )
      st.success("–ú–∞—Ç—Ä–∏—Ü–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞")

    #### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º
    st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞")
    option_method = st.radio("–ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
                              ("HDBSCAN", "HAC"),
                              horizontal=True)

    if st.button("–ù–∞—á–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"):
      if hasattr(st.session_state, "D"):
        st.session_state.labels = clusterize(
            X.iloc[st.session_state.idx_S],
            st.session_state.D,
            method=option_method
        )
      else:
        st.error("–†–∞—Å—Å—á–∏—Ç–∞–π—Ç–µ —Ä–∞—Å—Å—Ç–æ–Ω–Ω–∏—è –ì–æ–≤–µ—Ä–∞")

      # --- FAISS HNSW index –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ > 8000 ---
      if X.shape[0] > 8000:
        st.subheader("FAISS HNSW —Ä–∞–∑–º–µ—Ç–∫–∞ –ª–µ–π–±–ª–æ–≤")
        X_vectorized = vectorize(X, num_cols, cat_cols)
        X_S = X_vectorized[st.session_state.idx_S]
        X_rest = X_vectorized[st.session_state.idx_rest]
        index = build_faiss_hnsw(X_S, M=32, efSearch=64)

        # --- assign –æ—Å—Ç–∞–ª—å–Ω—ã—Ö ---
        labels_rest, dist_rest, is_ood = assign_faiss(index,
                                                      X_rest,
                                                      st.session_state.labels,
                                                      k=3
                                                      )

        # --- —Å–æ–±—Ä–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
        result = df.copy()
        result['cluster'] = np.nan
        result['assign_distance'] = np.nan
        result['is_OOD'] = False

        # –º–µ—Ç–∫–∏ –¥–ª—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏
        result.loc[st.session_state.idx_S, 'cluster'] = st.session_state.labels
        result.loc[st.session_state.idx_S, 'assign_distance'] = 0.0
        result.loc[st.session_state.idx_S, 'is_OOD'] = False

        # –º–µ—Ç–∫–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        result.loc[st.session_state.idx_rest, 'cluster'] = labels_rest
        result.loc[st.session_state.idx_rest, 'assign_distance'] = dist_rest
        result.loc[st.session_state.idx_rest, 'is_OOD'] = is_ood
        st.session_state.result = result
        st.success(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º {option_method} —Å –¥–æ—Ä–∞–∑–º–µ—Ç–∫–æ–π FAISS –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

      else:
        result = df.copy()
        st.write(f"–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è {len(result.head(500))} –∏–∑ {len(result)} —Å—Ç—Ä–æ–∫")
        st.dataframe(result.head(500))    
        result.loc[st.session_state.idx_S, 'cluster'] = st.session_state.labels
        st.session_state.result = result
        st.success(f"–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º {option_method} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    ##### –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    if hasattr(st.session_state, "result"):
      st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
      st.write(f"–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è {len(st.session_state.result.head(500))} –∏–∑ {len(st.session_state.result)} —Å—Ç—Ä–æ–∫")
      st.dataframe(st.session_state.result.head(500))

      st.header("–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
      if st.button("–ù–∞—á–∞—Ç—å –∞–Ω–∞–ª–∏–∑"):
        st.session_state.summary, st.session_state.super_tree, st.session_state.importances = (
            analyze_all_clusters(
                st.session_state.result,
                target,
                num_cols,
                cat_cols
                )
        )

        with NamedTemporaryFile(suffix=".html", delete=False) as f:
          st.session_state.super_tree.save_html(f.name)
          st.session_state.super_tree_html = open(f.name, "r", encoding="utf-8").read()

      if hasattr(st.session_state, "super_tree"):
        st.subheader("–ü—Ä–∞–≤–∏–ª–∞, –æ–±—ä—è—Å–Ω—è—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")

        st.text(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ {target} –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
        st.dataframe(st.session_state.summary)
        st.subheader("\n –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —Ñ–æ—Ä–º–∏—Ä—É—é—â–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:")
        st.dataframe(st.session_state.importances.head(10))
        if "super_tree_html" in st.session_state:
          html(st.session_state.super_tree_html, height=650)

      option_cluster = st.selectbox(
          "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –≤–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
          st.session_state.result['cluster'].unique()
      )
      if st.button("–í–Ω—É—Ç—Ä–∏–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑"):
        cluster_tree = analyze_within_clusters(
            st.session_state.result,
            target,
            num_cols,
            cat_cols,
            cluster=option_cluster
        )
        with NamedTemporaryFile(suffix=".html", delete=False) as f1:
          cluster_tree.save_html(f1.name)
          html(f1.read(), height=650)

else:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")