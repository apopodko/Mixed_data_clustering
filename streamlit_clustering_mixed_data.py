import pandas as pd
import numpy as np
import time
import random
import streamlit as st

# plots
import seaborn as sns
import matplotlib.pyplot as plt

# HAC
from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score

# HDBSCAN
import hdbscan
from hdbscan.validity import validity_index

# FeatureEncode
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction import FeatureHasher
from sklearn.preprocessing import LabelEncoder
#from category_encoders import CatBoostEncoder

# Feature weights
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression

# Cluster Explanation
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree

import faiss

# –†–∞—Å—á–µ—Ç —Ä–µ–Ω–∂–µ–π –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_numeric_ranges(df, num_cols, method='minmax', iqr_multiplier=1.5):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {col: range} –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
      - 'minmax'  : range = max - min
      - 'iqr' : range = (Q3-Q1) * iqr_multiplier
    """
    ranges = {}
    for c in num_cols:
        col = df[c].dropna().to_numpy(dtype=float)
        if col.size == 0:
            ranges[c] = 1.0
            continue

        if method == 'minmax':
            r = float(np.max(col) - np.min(col))
        elif method == 'iqr':
            q1 = np.percentile(col, 25)
            q3 = np.percentile(col, 75)
            r = float((q3 - q1) * iqr_multiplier)
        else:
            raise ValueError("unknown method")

        # –ï—Å–ª–∏ range —Å–ª–∏—à–∫–æ–º –º–∞–ª, —Ç–æ —Å—Ç–∞–≤–∏–º –µ–≥–æ 1.0
        if not np.isfinite(r) or r <= 1e-8:
            r = 1.0
        ranges[c] = r
    return ranges

# –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –¥–ª—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_feature_weights(df, target, num_cols, cat_cols, task="regression"):
    """
    –°—á–∏—Ç–∞–µ—Ç –≤–µ—Å–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –≤–∑–∞–∏–º–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (MI) —Å —Ç–∞—Ä–≥–µ—Ç–æ–º
    df : –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    target : pd.Series –∏–ª–∏ np.array —Ç–∞—Ä–≥–µ—Ç (—á–∏—Å–ª–æ–≤–æ–π –∏–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π)
    task : str
        "regression" –∏–ª–∏ "classification".

    output:
    weights : dict
        {column_name: weight}, –≤–µ—Å–∞ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã (—Å—É–º–º–∞ = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤).
    """
    df = df.copy()
    y = target.values if isinstance(target, pd.Series) else np.array(target)

    # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ
    df_enc = df.copy()
    discrete_features = []
    for c in cat_cols:
        le = LabelEncoder()
        df_enc[c] = le.fit_transform(df_enc[c].astype(str))
        discrete_features.append(df.columns.get_loc(c))  # –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö

    X = df_enc.to_numpy()

    # –í—ã—á–∏—Å–ª—è–µ–º MI
    if task == "regression":
        mi = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=42)
    elif task == "classification":
        mi = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=42)
    else:
        raise ValueError("task –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'regression' –∏–ª–∏ 'classification'")

    # –ù–æ—Ä–º–∏—Ä—É–µ–º –≤–µ—Å–∞ (—Å—É–º–º–∞ = —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)
    mi = np.maximum(mi, 1e-9)  # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –Ω—É–ª–µ–≤—ã—Ö
    mi_normalized = mi / mi.sum() * len(mi)

    weights = {col: w for col, w in zip(df.columns, mi_normalized)}
    return weights

# –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞
def compute_gower_matrix(df, num_cols, cat_cols, numeric_ranges=None,
                          weights=None, dtype=np.float32):
    """
    - numeric_ranges: dict col->range. –ï—Å–ª–∏ None, computed by minmax on df.
    - weights: dict col->weight. –ï—Å–ª–∏ None - –≤—Å–µ 1
    """
    X = df.reset_index(drop=True)
    n = len(X)
    if n == 0:
        return np.zeros((0,0), dtype=dtype)

    if numeric_ranges is None:
        numeric_ranges = compute_numeric_ranges(X, num_cols, method='minmax')

    D = np.zeros((n, n), dtype=np.float64)  # accumulate in float64 for numeric stability

    # weights
    if weights is None:
        # equal weight per feature
        w_num = {c: 1.0 for c in num_cols}
        w_cat = {c: 1.0 for c in cat_cols}
    else:
        # user-provided; missing keys default to 1.0
        w_num = {c: float(weights.get(c, 1.0)) for c in num_cols}
        w_cat = {c: float(weights.get(c, 1.0)) for c in cat_cols}

    # numeric part
    for c in num_cols:
        col = X[c].to_numpy(dtype=float)
        rng = numeric_ranges.get(c, 1.0)
        # normalized differences (broadcast)
        mat = np.abs(col[:, None] - col[None, :]) / rng
        D += w_num.get(c, 1.0) * mat

    # categorical part
    for c in cat_cols:
        col = X[c].astype(str).to_numpy()
        neq = (col[:, None] != col[None, :]).astype(np.float64)
        D += w_cat.get(c, 1.0) * neq

    # normalize by total weights sum
    total_weight = sum(w_num.values()) + sum(w_cat.values())
    if total_weight <= 0:
        total_weight = 1.0
    D = (D / float(total_weight)).astype(dtype)
    return D

### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
def tune_hdbscan(D, param_grid, d):
    best_score = -1
    best_params = None
    best_labels = None

    for params in param_grid:
        cl = hdbscan.HDBSCAN(metric="precomputed", **params)
        labels = cl.fit_predict(D)

        if len(set(labels)) > 1:
            score = validity_index(
                D,
                labels,
                metric="precomputed",
                d=d
            )
            if score > best_score:
                best_score = score
                best_params = params
                best_labels = labels
    return best_params, best_score, best_labels

def clusterize(df, D, method="hac", max_k=20):
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ.
    method = "hdbscan" –∏–ª–∏ "hac"
    """

    if method == "hdbscan":
        cl = hdbscan.HDBSCAN(
            metric="precomputed"
        )
        # specify parameters and distributions to sample from
        param_grid = [
            {"min_cluster_size": mcs, "min_samples": ms, "cluster_selection_method": csm}
            for mcs in [10, 20, 30, 50, 100]
            for ms in [5, 10, 20, 30, 50, 100]
            for csm in ["eom", "leaf"]
        ]
        d = df.shape[1]
        best_params, best_score, labels = tune_hdbscan(
            D.astype(np.float64), param_grid, d)

        print("Best params:", best_params)
        print("Best validity:", best_score)

    elif method == "hac":
        # –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤ condensed form
        condensed = squareform(D, checks=False)

        # linkage (average linkage –ª—É—á—à–µ –¥–ª—è Gower)
        Z = linkage(condensed, method="complete")

        # –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k –ø–æ silhouette
        best_k, best_score, best_labels = None, -1, None
        for k in range(2, min(max_k, len(df)//100)):
            labels = fcluster(Z, k, criterion="maxclust")
            if len(np.unique(labels)) < 2:
                continue
            try:
                score = silhouette_score(D, labels, metric="precomputed")
            except Exception:
                continue
            if score > best_score:
                best_score, best_k, best_labels = score, k, labels

        if best_labels is None:
            # fallback: –≤—Å—ë –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
            best_labels = np.ones(len(df), dtype=int)

        labels = best_labels

    else:
        raise ValueError("method must be 'hdbscan' or 'hac'")

    return labels

### –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è FAISS
def vectorize(df, num_cols, cat_cols, ohe_thresh=20, hash_dim=32):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –≤ —á–∏—Å–ª–æ–≤–æ–π –º–∞—Å—Å–∏–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ / –ø–æ–∏—Å–∫–∞ —Å–æ—Å–µ–¥–µ–π.

    Parameters:
        df : pd.DataFrame
        numeric_cols : list[str] - —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        cat_cols : list[str] - –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        ohe_thresh : int - –º–∞–∫—Å —á–∏—Å–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è OHE
        hash_dim : int - —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è FeatureHasher

    output:
        X : np.ndarray, float32
    """
    # --- —á–∏—Å–ª–æ–≤—ã–µ ---
    scaler = RobustScaler()
    X_num = scaler.fit_transform(df[num_cols]) if num_cols else None

    # --- –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ---
    X_cat_list = []
    encoders = {}

    for c in cat_cols:
        n_unique = df[c].nunique()
        col_data = df[[c]].astype(str)

        if n_unique <= ohe_thresh:
            # OHE
            enc = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            X_c = enc.fit_transform(col_data)
            encoders[c] = enc
        else:
            # FeatureHasher
            enc = FeatureHasher(n_features=hash_dim, input_type='string')
            # FeatureHasher –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: [{cat_val: 1}, ...]
            X_c = enc.transform([{val: 1} for val in col_data[c]]).toarray()
            encoders[c] = enc
        X_cat_list.append(X_c)

    if X_cat_list:
        X_cat = np.hstack(X_cat_list)
    else:
        X_cat = None

    # --- –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ---
    if X_num is not None and X_cat is not None:
        X = np.hstack([X_num, X_cat])
    elif X_num is not None:
        X = X_num
    else:
        X = X_cat

    return X.astype(np.float32)

######################## Streamlit layout ##################

st.set_page_config(page_title="–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö", layout="wide")

st.title("–ê–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ ===
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel —Ñ–∞–π–ª", type=["csv", "xlsx"])
with st.expander("–ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–æ—Ä–º–∞—Ç–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º"):
  option_decimal = st.radio(
      "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ—Å—è—Ç–∏—á–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å",
      (".", ","),
      horizontal=True)
  option_sep = st.text_input("–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π —Ç–∏–ø —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –¥–ª—è CSV", value=",")

if uploaded_file is not None:
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏ —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª

    if uploaded_file.name.endswith(".csv"):
        if option_decimal == ".":
            option_thousand = ','
        else:
            option_thousand = None
        try:
          df = pd.read_csv(
              uploaded_file,
              sep=option_sep,
              decimal=option_decimal,
              thousands=option_thousand)
        except (ValueError, TypeError):
          st.error('–ü—Ä–æ–±–ª–µ–º–∞ –≤ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞', icon="üö®")
    else:
        df = pd.read_excel(uploaded_file)

    # === 2. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ===
    for col in df.select_dtypes(exclude=[np.number]).columns:
        try:
            df[col] = pd.to_datetime(df[col])
        except (ValueError, TypeError):
            pass  # –Ω–µ –¥–∞—Ç–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

    st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö —Ç–∏–ø—ã")
    st.dataframe(df.head())
    st.dataframe(df.dtypes.to_frame().T)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    st.subheader("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")

    # === 3. –í—ã–±–æ—Ä —Ç–∞—Ä–≥–µ—Ç–∞ ===
    target = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫", df.columns.tolist())
    df.dropna(subset=[target], inplace=True)
    option_task = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞",
                           ("–ß–∏—Å–ª–æ–≤–æ–π", "–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π"),
                           horizontal=True)
    if option_task == "–ß–∏—Å–ª–æ–≤–æ–π":
      option_task = "regression"
    else:
      option_task = "classification"

    # === 4. –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è ===
    drop_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è",
                               df.drop(target, axis=1).columns.tolist())
    if drop_cols:
        df = df.drop(columns=drop_cols)

    # === 5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è ===
    filter_cols = st.multiselect("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞", df.columns.tolist())
    if filter_cols:

      with st.expander("–û—Ç–∫—Ä—ã—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã"):
          for col in filter_cols:
              col_type = df[col].dtype

              if pd.api.types.is_datetime64_any_dtype(col_type):
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
                  min_date, max_date = df[col].min(), df[col].max()
                  start_date, end_date = st.date_input(
                      f"{col}: –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç",
                      [min_date, max_date],
                      key=col
                  )
                  mask = (df[col] >= pd.to_datetime(start_date)) & (df[col] <= pd.to_datetime(end_date))
                  df = df.loc[mask]

              elif pd.api.types.is_numeric_dtype(col_type):
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ —á–∏—Å–ª–æ–≤–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
                  min_val, max_val = float(df[col].min()), float(df[col].max())
                  val_range = st.slider(
                      f"{col}: –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π",
                      min_val, max_val, (min_val, max_val),
                      key=col
                  )
                  mask = (df[col] >= val_range[0]) & (df[col] <= val_range[1])
                  df = df.loc[mask]

              else:
                  # —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º/—Ç–µ–∫—Å—Ç—É
                  unique_vals = df[col].dropna().unique().tolist()
                  if len(unique_vals) > 1 and len(unique_vals) <= 100:  # –æ–≥—Ä–∞–Ω–∏—á–∏–º —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —Å–ø–∏—Å–∫–∏
                      selected_vals = st.multiselect(
                          f"{col}: –≤—ã–±–µ—Ä–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è",
                          unique_vals,
                          default=unique_vals,
                          key=col
                      )
                      df = df[df[col].isin(selected_vals)]

    # === 6. –†–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ ===
    missing_option = st.radio("–ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ –¥–∞–Ω–Ω—ã—Ö?",
      ("–ó–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π –∏ MISSING", "–í—ã–±—Ä–æ—Å–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏"),
      horizontal=True)
    num_cols = (df
                .drop(target, axis=1)
                .select_dtypes(include=[np.number])
                .columns.tolist())
    cat_cols = (df
                .drop(target, axis=1)
                .select_dtypes(exclude=[np.number, np.datetime64])
                .columns.tolist())

    if missing_option == "–ó–∞–ø–æ–ª–Ω–∏—Ç—å –º–µ–¥–∏–∞–Ω–æ–π –∏ MISSING":
      # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π, –ª–∏–±–æ __MISSING__
      for col in num_cols:
          df[col] = df[col].fillna(df[col].median())
      for col in cat_cols:
          df[col] = df[col].fillna("__MISSING__").astype(str)
    else:
      # –í—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
      df = df.dropna()

    # === 7. –†–µ–∑—É–ª—å—Ç–∞—Ç ===
    st.subheader("–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    st.write(f"–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è {len(df)} –∏–∑ {len(df)} —Å—Ç—Ä–æ–∫")
    st.dataframe(df)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    X = df.drop(target, axis=1)
    y = df[target]


    ############# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ################################
    st.header("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
    ### –°—á–∏—Ç–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –ì–æ–≤–µ—Ä–∞
    st.subheader("–†–∞—Å—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞")

    option_weights = st.radio("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–Ω–æ–π –∏—Ñ–Ω–æ—Ä–º–∞—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–æ–º?",
                              ("–î–∞", "–ù–µ—Ç"),
                              horizontal=True)
    
    if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞"):


      # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è FAISS  
      X_vectorized = vectorize(X, num_cols, cat_cols)
      N = X.shape[0]
      S = min(8000, N)
      idx_all = np.arange(N)
      idx_S = np.random.choice(idx_all, size=S, replace=False)
      idx_rest = np.setdiff1d(idx_all, idx_S)

      X_S = X_vectorized[idx_S]
      X_rest = X_vectorized[idx_rest]

      if option_weights == "–î–∞":
        weights = compute_feature_weights(X.iloc[idx_S], y.iloc[idx_S], num_cols, cat_cols, task=option_task)
      else:
        weights = None       
      st.session_state.idx_S = idx_S
      st.session_state.D = compute_gower_matrix(X.iloc[idx_S], num_cols, cat_cols, numeric_ranges=None, weights=weights)
      st.text(weights)

    #### –ö–∞–ª—Å—Ç–µ—Ä–∏–∑—É–µ–º
    st.subheader("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –ì–æ–≤–µ—Ä–∞")
    option_method = st.radio("–ú–µ—Ç–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
                              ("HDBSCAN", "HAC"),
                              horizontal=True)
    
    if st.button("–ù–∞—á–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"):

      if option_method=="HDBSCAN":
        option_method=="hdbscan"
      else:
        option_method=="hac"
      labels = clusterize(X.iloc[st.session_state.idx_S], st.session_state.D, method="hac", max_k=20)
      st.text(labels)
else:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")